{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Edit Metadata",
    "colab": {
      "name": "absorp_ratio - autoencoder-vs-pca-decomposition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "machine-learning-in-finance"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andtab/absorption-ratio-trading-strategy/blob/main/absorp_ratio_autoencoder_vs_pca_decomposition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b21cnexLbFn"
      },
      "source": [
        "Predicting Systemic Shock via Absorption Ratio Using Decomposition by PCA and AE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrSAdLrSp59n"
      },
      "source": [
        "Using PCA and autoencoder decomposition methods to predict system shock via Absorption Ratio. Based on the following paper linked below: <br>\n",
        "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.466.5471&rep=rep1&type=pdf\n",
        "\n",
        "Accompanying presentation can be found here:\n",
        "https://docs.google.com/presentation/d/1VEUksJE7PvS3WN7hw2w7jZnvFYBALHFWJ8kCWMqxkas/edit#slide=id.p20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTTDqRx7kcqc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.decomposition\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection\n",
        "\n",
        "\n",
        "# Install factor analyzer on Google Colab\n",
        "!pip install factor_analyzer\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "\n",
        "# AT - Additional imports\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBeIen_vkcqi"
      },
      "source": [
        "#1. Go to WRDS CRSP database and obtain the price and ticker data for the following industry ETFs:\n",
        "#    IYW IYJ IBB IGV IXP IYT VNQ PBS PBJ KCE KIE KBE XSD ITB ITA IHI IHF IHE IEZ FDN\n",
        "#2. set the starting and ending dates as follows: 1/3/2007 and 12/31/2019\n",
        "\n",
        "#3. Unstack the prices and save the unstacked price data into a csv file called:\n",
        "#    20industries_ETF_prc_UNSTACKED.csv\n",
        "\n",
        "#UNSTACK PRICES\n",
        "\n",
        "StackedPrices = pd.read_csv('20industries_ETF_prc_STACKED.csv')\n",
        "StackedPrices.date = pd.to_datetime(StackedPrices.date)\n",
        "StackedPrices.PRC = StackedPrices.PRC.abs()\n",
        "StackedPrices.drop(['PERMNO'], axis=1, inplace=True)\n",
        "UnstackedPrices = pd.pivot_table(StackedPrices, values = 'PRC', index = 'date', columns = 'TICKER')\n",
        "UnstackedPrices.sort_index(inplace=True)\n",
        "print(UnstackedPrices.shape)\n",
        "UnstackedPrices.head(5)\n",
        "UnstackedPrices.to_csv('20industries_ETF_prc_UNSTACKED.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cj8yKD7kcqk"
      },
      "source": [
        "\n",
        "# load dataset\n",
        "\n",
        "asset_prices = pd.read_csv('20industries_ETF_prc_UNSTACKED.csv',\n",
        "                     date_parser=lambda dt: pd.to_datetime(dt, format='%Y-%m-%d'),\n",
        "                     index_col = 0).dropna()\n",
        "n_stocks_show = 20\n",
        "print('Asset prices shape', asset_prices.shape)\n",
        "asset_prices.iloc[:, :n_stocks_show].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQuxgTPpkcqn"
      },
      "source": [
        "### Calculate daily returns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NByj0fOvkcqo"
      },
      "source": [
        "asset_returns = asset_prices.pct_change(periods=1)\n",
        "asset_returns = asset_returns.iloc[1:, :]\n",
        "asset_returns.iloc[:, :n_stocks_show].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G53mtCxFkcqq"
      },
      "source": [
        "\n",
        "def normalize_returns(r_df):\n",
        "    \"\"\"\n",
        "    Normalize, i.e. center and divide by standard deviation raw asset returns data\n",
        "\n",
        "    Arguments:\n",
        "    r_df -- a pandas.DataFrame of asset returns\n",
        "\n",
        "    Return:\n",
        "    normed_df -- normalized returns\n",
        "    \"\"\"\n",
        "\n",
        "    mean_r = r_df.mean()\n",
        "    sd_r = r_df.std()\n",
        "    normed_df = (r_df - mean_r)/sd_r\n",
        "    \n",
        "\n",
        "    return normed_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kg1AwVwkcqr"
      },
      "source": [
        "normed_r = normalize_returns(asset_returns)\n",
        "normed_r.iloc[:, :n_stocks_show].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWR4Squ2kcqs"
      },
      "source": [
        "#### Part 1 (Implement an exponentially-weighting function to mimic pandas.EWM function)\n",
        "Define sequence of $X_j$ where $j \\subset [0, N]$, an integer taking all values in the interval from 0 to N  $$ X_j =  e^{-\\frac{log(2)}{H} \\times  \\space j}$$\n",
        "where H is half-life which determines the speed of decay, and $log$ is natural log function.\n",
        "Then a sequence of exponentially decaying weights $w_j$ is defined as: $$ w_j = \\frac{X_j}{ \\sum\\limits_{i=0}^j X_i } $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLaPEIWLkcqt"
      },
      "source": [
        "#Exponent weighting function, to take the place of Pandas EWM function (we will compare their effect)\n",
        "def exponent_weighting(n_periods, half_life = 252):\n",
        "    \"\"\"\n",
        "    Calculate exponentially smoothed normalized (in probability density function sense) weights\n",
        "\n",
        "    Arguments:\n",
        "    n_periods -- number of periods, an integer, N in the formula above\n",
        "    half_life -- half-life, which determines the speed of decay, h in the formula\n",
        "    \n",
        "    Return:\n",
        "    exp_probs -- exponentially smoothed weights, np.array\n",
        "    \"\"\"\n",
        "    \n",
        "    #exp_probs = np.zeros(n_periods) # do your own calculation instead of dummy zero array\n",
        "\n",
        "\n",
        "    arr= np.array(range(0,n_periods))\n",
        "    num= np.exp(-np.log(2) * arr / half_life)\n",
        "    denom= np.cumsum(num)\n",
        "    exp_probs = num / denom\n",
        "    exp_probs = np.flip(exp_probs)\n",
        "\n",
        "    \n",
        "    return exp_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zHD5QTYkcqv"
      },
      "source": [
        "#note the flipping for graphing purposes only, \n",
        "#the graph should look like an exponential probability density function\n",
        "#in reality we want weights applied to returns \n",
        "#to decay as returns are older, further in the past\n",
        "exp_probs = exponent_weighting(252*1)\n",
        "plt.plot(np.flip(exp_probs), linewidth=3) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQgDGQsMkcqw"
      },
      "source": [
        "def absorption_ratio(explained_variance_arr, n_components):\n",
        "    \"\"\"\n",
        "    Calculate absorption ratio via PCA or Autoencoder.  \n",
        "    \n",
        "    Arguments:\n",
        "    explained_variance_arr -- 1D np.array of explained variance by each pricincipal component, in descending order\n",
        "    n_components -- an integer, a number of principal components to compute absorption ratio\n",
        "    Formally, the ratio is defined by the variance absorbed by the first n eigenvectors\n",
        "    The numerator equals the cumulative variance explained by the first n eigenvectors (=n_components) \n",
        "    And the denominator equals the total variance\n",
        "    Return:\n",
        "    ar -- absorption ratio\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ar = np.cumsum(explained_variance_arr)[n_components - 1]\n",
        "\n",
        "    return ar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZvC55-Xkcqy"
      },
      "source": [
        "stock_tickers = asset_returns.columns.values[:]\n",
        "assert 'SPX' not in stock_tickers, \"By accident included SPX index\"\n",
        "\n",
        "half_life = 252             # in (days)\n",
        "lookback_window = 252 * 1   # in (days)\n",
        "num_assets = len(stock_tickers)\n",
        "step_size = 1          # days : 5 - weekly, 21 - monthly, 63 - quarterly\n",
        "\n",
        "# require 0.8 variance to be explained. How many components are needed?\n",
        "var_threshold = 0.8     \n",
        "\n",
        "# fix 20% of principal components for absorption ratio calculation. How much variance (in %) do they explain?\n",
        "absorb_comp = int((1 / 5) * num_assets)  \n",
        "\n",
        "print('Half-life = %d' % half_life)\n",
        "print('Lookback window = %d' % lookback_window)\n",
        "print('Step size = %d' % step_size)\n",
        "print('Variance Threshold = ', var_threshold)\n",
        "print('Number of assets = %d' % num_assets)\n",
        "print('Number of principal components = %d' % absorb_comp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01u5k-2Ekcqz"
      },
      "source": [
        "# indexes date on which to compute PCA\n",
        "days_offset = 4 * 252 \n",
        "num_days = 6 * 252 + days_offset \n",
        "components_ts_index = asset_returns.index[list(range(lookback_window + days_offset, min(num_days, len(asset_returns)), step_size))]\n",
        "\n",
        "# allocate arrays for storing absorption ratio\n",
        "components = np.array([np.nan]*len(components_ts_index))\n",
        "absorp_ratio = np.array([np.nan]*len(components_ts_index))\n",
        "kmo = np.array([np.nan]*len(components_ts_index))\n",
        "\n",
        "exp_probs = exponent_weighting(lookback_window, half_life)\n",
        "assert 'SPX' not in asset_returns.iloc[:lookback_window, :].columns.values, \"By accident included SPX index\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoJkVF17OYqF"
      },
      "source": [
        "#1. Go to WRDS CRSP database and obtain the Holding Period Return and ticker for the following industry ETFs:\n",
        "#    VTI AGG\n",
        "#the holding period return includes dividends, coupon flows etc.\n",
        "#2. set the starting and ending dates as follows: 1/3/2007 and 12/31/2019\n",
        "#3. Unstack the holding period returns and save the unstacked returns data into a csv file called:\n",
        "#    AGG_VTI_UNSTACKED_adjusted_returns.csv\n",
        "\n",
        "#UNSTACK RETURNS\n",
        "StackedReturns = pd.read_csv('AGG_VTI_STACKED_adjusted_returns.csv')\n",
        "StackedReturns.date = pd.to_datetime(StackedReturns.date)\n",
        "StackedReturns.drop(['PERMNO'], axis=1, inplace=True)\n",
        "UnstackedReturns = pd.pivot_table(StackedReturns, values = 'RET', index = 'date', columns = 'TICKER')\n",
        "UnstackedReturns.sort_index(inplace=True)\n",
        "print(UnstackedReturns.shape)\n",
        "UnstackedReturns.to_csv('AGG_VTI_UNSTACKED_adjusted_returns.csv')\n",
        "UnstackedReturns.head(5)\n",
        "\n",
        "\n",
        "#These are adjusted PERCENT returns of AGG and VTI from WRDS CRSP, including distibutions, coupons, dividends etc.\n",
        "etf_percent_returns = pd.read_csv('AGG_VTI_UNSTACKED_adjusted_returns.csv')\n",
        "etf_percent_returns[\"date\"]=pd.to_datetime(etf_percent_returns['date'])\n",
        "etf_percent_returns.set_index(['date'], inplace=True)\n",
        "#Use the adjusted PERCENT returns to calculate the etf_log returns:\n",
        "\n",
        "etf_log_returns = np.log(1 + etf_percent_returns)\n",
        "#decide which return you are going to use by uncommenting a or b below (only a or b is correct, not both):\n",
        "#a:omp\n",
        "#etf_returns = etf_log_returns.copy()\n",
        "#b:\n",
        "etf_returns = etf_percent_returns.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyM_YT_uSMz1"
      },
      "source": [
        "\n",
        "def get_weight(ar_delta):\n",
        "    '''\n",
        "    Calculate EQuity / FIncome portfolio weights based on Absorption Ratio delta\n",
        "    Arguments:\n",
        "    ar_delta -- Absorption Ratio delta\n",
        "    \n",
        "    Return: \n",
        "        wgts -- a vector of portfolio weights\n",
        "    '''\n",
        "\n",
        "\n",
        "    if ar_delta < -1:\n",
        "        wgts = [0.5, 0.5]\n",
        "    elif ar_delta > 1:\n",
        "        wgts = [0.0, 1.0]\n",
        "    else:\n",
        "        wgts = [1.0, 0.0]\n",
        "    return wgts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWxh-0f1SPYp"
      },
      "source": [
        "\n",
        "def backtest_strategy(strat_wgts, asset_returns, periods_per_year = 252):\n",
        "    '''\n",
        "    Calculate portfolio returns and return portfolio strategy performance\n",
        "    Arguments:\n",
        "    \n",
        "    strat_wgts -- pandas.DataFrame of weights of the assets\n",
        "    asset_returns -- pandas.DataFrame of asset returns\n",
        "    periods_per_year -- number of return observations per year\n",
        "    \n",
        "    Return: \n",
        "        (ann_ret, ann_vol, sharpe) -- a tuple of (annualized return, annualized volatility, sharpe ratio)\n",
        "    before returning plot the cumulative return of the EWM+AGG portfolio  \n",
        "    '''\n",
        "\n",
        "    df_all=pd.merge(strat_wgts,asset_returns, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "    df_all['EQ_part'] = df_all['EQ'] * df_all['VTI']\n",
        "    df_all['FI_part'] = df_all['FI'] * df_all['AGG']\n",
        "    df_all['PORT_ret'] = df_all['EQ_part'] + df_all['FI_part'] \n",
        " \n",
        "    annualized_return= (1 + df_all['PORT_ret'].mean())**periods_per_year - 1\n",
        "    annualized_vol = df_all['PORT_ret'].std() * np.sqrt(periods_per_year)\n",
        "    annualized_sharpe = (df_all['PORT_ret'].mean()/df_all['PORT_ret'].std()) * np.sqrt(periods_per_year)\n",
        "    #before returning plot the cumulative return of the EWM+AGG portfolio \n",
        "    df_all['Cum_PORT_ret'] = (df_all['PORT_ret']).cumsum() + 1\n",
        "\n",
        "    returns = df_all['Cum_PORT_ret']\n",
        "\n",
        "    return annualized_return, annualized_vol, annualized_sharpe, returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxJg6rE-R_S2"
      },
      "source": [
        "# The goal of this function is to perform dimension reduction on the features\n",
        "# using either Principal Component Analysis (PCA) (a process for which sklearn \n",
        "# has useful functions) or an Autoencoder (AE) \n",
        "# (where eigen-decomposition must be built \"from scratch\")\n",
        "\n",
        "# The primary inputs of this function will be:\n",
        "# 1: the dataframe for which decomposition must be performed, and\n",
        "# 2: the decomposition method selected.\n",
        "\n",
        "# The outputs of this function will be: \n",
        "# 1: the latent features, in order of descending corresponding explained \n",
        "# variance, built from eigen-decomposition using either AE or PCA, and\n",
        "# 2: the similarly ordered explained variance ratios themselves\n",
        "def decomposition_function(input_dataframe, method = 'AE', \\\n",
        "                           node_count_list = None, max_encode = False):\n",
        "\n",
        "  # Convert the input dataframe into an array, for guaranteed compatibility\n",
        "  # with numpy operations\n",
        "  input_data = input_dataframe.values\n",
        "\n",
        "\n",
        "\n",
        "  # If selected decomposition method is 'AE', standing for \"Autoencoder\", the \n",
        "  # procedure contained within this block of the IF statement will be performed\n",
        "  if method == 'AE':\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    # These are hyperparameters related to the training process of the AE model \n",
        "    num_epochs = 20 # Number of passes through the dataset for training purposes\n",
        "\n",
        "    batch_size = 32 # The number of samples that must be observed before the \n",
        "                    # model goes through with an update\n",
        "\n",
        "    act = 'ReLU' # The type of activation function used with hidden and input layers\n",
        "\n",
        "    final_act = 'linear' # The type of activation function used just after output layer\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate= 0.0005) # The optimizer used \n",
        "                       # to update model performance after \n",
        "                       # each epoch. Learning rate can be set here\n",
        "\n",
        "    loss = 'mean_squared_error' # The loss which the optimizer attempts to \n",
        "                                # minimize through each pass of the dataset\n",
        "\n",
        "    # The immediate input dimensions for the Autoencoder model; this is the \n",
        "    # number of features/columns used for prediction input in our dataset\n",
        "    input_dim = input_data.shape[1]\n",
        "\n",
        "    # Create the specified number of hidden layers in our network, each having\n",
        "    # a specified number of nodes \n",
        "    for x in range(1, len(node_count_list) + 1):\n",
        "      model.add(Dense(units = node_count_list[x - 1], activation=act, \\\n",
        "                      input_dim = input_dim, name= 'hidden_layer_' + str(x), \\\n",
        "                    activity_regularizer = regularizers.L1L2(10e-5,10e-5)))\n",
        "\n",
        "    # Add an output layer to the model\n",
        "    model.add(Dense(units = input_dim, activation = final_act))\n",
        "    \n",
        "    # Compile the model, the final step in generating a model structure that\n",
        "    # is ready to be fit and trained on a dataset\n",
        "    model.compile(optimizer = optimizer,\n",
        "                  loss = loss)\n",
        "\n",
        "    # Fit the model to the dataset\n",
        "    history = model.fit(x = input_data, y = input_data,\n",
        "                    epochs = num_epochs,\n",
        "                    batch_size = batch_size,\n",
        "                    shuffle = False,\n",
        "                    validation_data = (input_data, input_data),\n",
        "                    verbose = 0)\n",
        "    \n",
        "    # When the max_encode parameter is set to 'True', this represents encoding \n",
        "    # the features space to the minimum possible dimension based on the \n",
        "    # autoencoder architecture. Otherwise, the entire hidden layer architecture\n",
        "    # is used for decomposition\n",
        "    input_layer_name = 'hidden_layer_1'\n",
        "\n",
        "    if max_encode == False:\n",
        "      output_layer_name = 'hidden_layer_' + str(x)\n",
        "    \n",
        "    else:\n",
        "      \n",
        "      # Reverse the list of node counts and determine the hidden layer with \n",
        "      # the minimum node count\n",
        "      node_count_list.reverse()\n",
        "      node_count, idx = min((node_count, idx) for (idx, node_count) in enumerate(node_count_list))\n",
        "      layer_number = len(node_count_list) - idx\n",
        "      output_layer_name = 'hidden_layer_' + str(layer_number)\n",
        "\n",
        "\n",
        "    # Create the intermediate layer model, containing only hidden layers\n",
        "    intermediate_layer_model = Model(inputs = model.get_layer(input_layer_name).input, outputs = model.get_layer(output_layer_name).output)\n",
        "\n",
        "    # Extract the latent features in column vector form \n",
        "    # from the intermediate layer model\n",
        "    latent_features = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    # Determine the explained variance of each feature in the order they are \n",
        "    # presented - they will eventually have to be ordered in terms of variance\n",
        "    # captured in the original data. This explained variance can be found by\n",
        "    # determining the covariance matrix of the latent feature matrix where \n",
        "    # each row represents one latent feature\n",
        "    explained_variance_unordered = np.diag(np.cov(latent_features.T))\n",
        "\n",
        "    # Calculate the total variance captured by each latent feature\n",
        "    total_variance = sum(explained_variance_unordered)\n",
        "\n",
        "    # Now we order the explained variance of each feature, but return only the\n",
        "    # indices. These indices are used to properly order the latent features\n",
        "    # according to the magnitude of their corresponding explained variance\n",
        "    explained_variance_ratio_ordered_idx = explained_variance_unordered.argsort()[::-1]  \n",
        "\n",
        "    # Now order the fraction of explained variance taken up by each latent \n",
        "    # feature from highest to lowest\n",
        "    explained_variance_ratio_ordered = \\\n",
        "      explained_variance_unordered[explained_variance_ratio_ordered_idx] / total_variance\n",
        "\n",
        "    # Order the latent features according their explained variance from \n",
        "    # highest to lowest\n",
        "    latent_features_ordered = latent_features[:, explained_variance_ratio_ordered_idx]\n",
        "    \n",
        "    # Reset model weights after training on the current dataset has been \n",
        "    # completed, prior to moving on to the next dataset\n",
        "    model.reset_states()\n",
        "\n",
        "################################################################################################\n",
        "  # If PCA rather than AE is being applied for decomposition, the following \n",
        "  # portion of the IF statement will be executed\n",
        "  else:\n",
        "    \n",
        "    # Instantiate PCA class\n",
        "    pca = PCA()\n",
        "    \n",
        "    # Fit PCA instance to the input data\n",
        "    pca.fit(input_data)\n",
        "\n",
        "    # Extract the ordered explained variance ratios and latent features \n",
        "    # corresponding to those explained variance ratios\n",
        "    explained_variance_ratio_ordered = pca.explained_variance_ratio_\n",
        "    latent_features_ordered = np.matmul(input_data, pca.components_)\n",
        "\n",
        "  # Return explained variance ratios and latent features\n",
        "  return explained_variance_ratio_ordered, latent_features_ordered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGXsQ_SFOJwe"
      },
      "source": [
        "sharpe_ratio_AR = []\n",
        "sharpe_ratio_EW = []\n",
        "\n",
        "ann_ret_AR = []\n",
        "ann_ret_EW = []\n",
        "\n",
        "volatility_AR = []\n",
        "volatility_EW = []\n",
        "\n",
        "returns_AR_list = []\n",
        "returns_EQ_list = []\n",
        "\n",
        "\n",
        "AR_calculation_time = []\n",
        "\n",
        "ts_absorb_ratio_list = []\n",
        "ts_components_list = []\n",
        "\n",
        "#********ADJUSTABLE HYPERPARAMETERS*********************************************\n",
        "\n",
        "# Select a method of decomposition - either 'PCA' or 'AE'\n",
        "method = 'AE'\n",
        "\n",
        "# comp_limit: maximum number of components used to calculate the AR \n",
        "# such that (comp_limit < total # of features post-decomposition)\n",
        "comp_limit = 9\n",
        "\n",
        "# List of the number of components used to calculate AR in each iteration\n",
        "comp_iteration = list(range(1, comp_limit + 1))\n",
        "\n",
        "\n",
        "\n",
        "# List of nodes required for each hidden layer of the AE model - list length\n",
        "# is the number of hidden layers the model will have. Note that if this list\n",
        "# is set and the decomposition method is set to 'PCA', it will have no effect\n",
        "node_count_list = [15, 10, 15]\n",
        "\n",
        "# Parameter which dictates which strategy for using hidden autoencoder layers \n",
        "# is applied to the decomposition process. If \"False\", all hidden layers will be\n",
        "# used to decompose the feature space; \n",
        "max_encode = False\n",
        "\n",
        "#*******************************************************************************\n",
        "\n",
        "for comp in comp_iteration:\n",
        "\n",
        "  ik = 0\n",
        "  ewm = True # setting ewm to false means historical returns are equally weighted\n",
        "  manual_ewm = True  #use manual ewm (instead of pandas ewm function)\n",
        "  #use_cov_mat = False #use the square shaped covariance of ret_frame instead of t ret_frame directly\n",
        "\n",
        "  lin_ae = None\n",
        "  time_start = time.time()\n",
        "  for ix in range(lookback_window + days_offset, min(num_days, len(asset_returns)), step_size):\n",
        "      ret_frame = normalize_returns(asset_returns.iloc[ix - lookback_window:ix, :]) # fixed window \n",
        "      ret_frame_kmo = ret_frame.copy(deep=True)\n",
        "      if ewm:\n",
        "\n",
        "          if manual_ewm:\n",
        "              #multiply the returns of ret_frame by the ex_probs (hint:transposing is required more than once)\n",
        "              #hint: make sure smallest weights multiply the oldest returns\n",
        "              ret_frame = (ret_frame.T * exp_probs).T\n",
        "    \n",
        "          else:\n",
        "              ret_frame = ret_frame.ewm(halflife=252).mean() #for comparison\n",
        "\n",
        "      \n",
        "      if ik == 0 or ik % 21 == 0:\n",
        "\n",
        "          ### fit decomposition method of choice (AE or PCA), compute the number of\n",
        "          # required latent features/components to meet the variance threshold, \n",
        "          # and absorption ratio \n",
        "          # store results into components[ik] and absorp_ratio[ik]\n",
        "\n",
        "          explained_variance, zfeatures = decomposition_function(ret_frame, method = method, node_count_list = node_count_list, max_encode = max_encode)\n",
        "              \n",
        "          #Using ret_frame_kmo, calculate the KMO statistic (=kmo_total) and assign it to kmo[ik]. \n",
        "          kmo_per_variable, kmo_total = calculate_kmo(ret_frame_kmo)\n",
        "          kmo[ik] = kmo_total \n",
        "\n",
        "          components[ik] = np.sum(np.cumsum(explained_variance) < var_threshold) + 1\n",
        "          \n",
        "          #calculate the absorption ratio by calling absorption_ratio() and assign it to absorp_ratio[ik]\n",
        "          #when you run this loop try various settings for the parameter n_components in this absorption_ratio function.\n",
        "          absorp_ratio[ik] = absorption_ratio(explained_variance, comp)\n",
        " \n",
        "      else:\n",
        "          absorp_ratio[ik] = absorp_ratio[ik-1] \n",
        "          components[ik] = components[ik-1]\n",
        "          kmo[ik] = kmo[ik-1]\n",
        "\n",
        "      ik += 1\n",
        "  runtime = time.time() - time_start\n",
        "  print ('Absorption Ratio done! Time elapsed: {} seconds'.format(runtime))   \n",
        "\n",
        "  AR_calculation_time.append(runtime) \n",
        "\n",
        "  ts_components = pd.Series(components, index=components_ts_index)\n",
        "  ts_absorb_ratio = pd.Series(absorp_ratio, index=components_ts_index)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #np.amin(kmo)\n",
        "\n",
        "  # ts_absorb_ratio.plot(figsize=(12,6), title='Absorption Ratio via PCA', linewidth=3)\n",
        "  # plt.savefig(\"Absorption_Ratio_20i.png\", dpi=900)\n",
        "\n",
        "  #pd.DataFrame(components).plot()\n",
        "\n",
        "  # following Kritzman and computing AR_delta = (15d_AR -1yr_AR) / sigma_AR\n",
        "  ts_ar = ts_absorb_ratio\n",
        "  ar_mean_1yr = ts_ar.rolling(252).mean()\n",
        "  ar_mean_15d = ts_ar.rolling(15).mean()\n",
        "  ar_sd_1yr = ts_ar.rolling(252).std()\n",
        "  ar_delta = (ar_mean_15d - ar_mean_1yr) / ar_sd_1yr  # standardized shift in absorption ratio\n",
        "\n",
        "  # df_plot = pd.DataFrame({'AR_delta': ar_delta.values, 'AR_1yr': ar_mean_1yr.values, 'AR_15d': ar_mean_15d.values}, \n",
        "  #                        index=ts_ar.index)\n",
        "  # df_plot = df_plot.dropna()\n",
        "  # if df_plot.shape[0] > 0:\n",
        "  #     df_plot.plot(figsize=(12, 6), title='Absorption Ratio Delta', linewidth=3, secondary_y=['AR_1yr','AR_15d'])\n",
        "\n",
        "  #Average trades per year\n",
        "  ar_delta_data = ar_delta[251:]\n",
        "\n",
        "  rebal_dates = np.zeros(len(ar_delta_data))\n",
        "  wgts = pd.DataFrame(data=np.zeros((len(ar_delta_data.index), 2)), index=ar_delta_data.index, columns=('EQ', 'FI'))\n",
        "\n",
        "  prtf_wgts = get_weight(ar_delta_data.values[0])\n",
        "  wgts.iloc[0, :] = prtf_wgts\n",
        "  for ix in range(1, len(ar_delta_data)):\n",
        "      prtf_wgts = get_weight(ar_delta_data.values[ix])\n",
        "      wgts.iloc[ix, :] = prtf_wgts\n",
        "      if wgts.iloc[ix-1, :][0] != prtf_wgts[0]:\n",
        "          prtf_wgts = wgts.iloc[ix, :]\n",
        "          rebal_dates[ix] = 1\n",
        "\n",
        "  ts_rebal_dates = pd.Series(rebal_dates, index=ar_delta_data.index)\n",
        "  ts_trades_per_year = ts_rebal_dates.groupby([ts_rebal_dates.index.year]).sum()\n",
        "  print('Average number of trades per year %.2f' % ts_trades_per_year.mean())\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "  ann_ret, ann_vol, sharpe, returns_AR = backtest_strategy(wgts, etf_returns)\n",
        "  print('Absorption Ratio strategy:', ann_ret, ann_vol, sharpe)\n",
        "\n",
        "  returns_AR_list.append(returns_AR)\n",
        "\n",
        "  ts_components_list.append(ts_components.copy())\n",
        "  ts_absorb_ratio_list.append(ts_absorb_ratio.copy())\n",
        "\n",
        "  ann_ret_AR.append(ann_ret)\n",
        "  volatility_AR.append(ann_vol)\n",
        "  sharpe_ratio_AR.append(sharpe)\n",
        "\n",
        "  eq_wgts = wgts.copy()\n",
        "  eq_wgts.iloc[:, ] = 0.5\n",
        "  ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt, returns_EQ = backtest_strategy(eq_wgts, etf_returns)\n",
        "  print('Equally weighted:', ann_ret_eq_wgt, ann_vol_eq_wgt, sharpe_eq_wgt)\n",
        "\n",
        "  ann_ret_EW.append(ann_ret_eq_wgt)\n",
        "  volatility_EW.append(ann_vol_eq_wgt)\n",
        "  sharpe_ratio_EW.append(sharpe_eq_wgt)\n",
        "\n",
        "  returns_EQ_list.append(returns_EQ)\n",
        "\n",
        "\n",
        "for x in range(0, len(returns_AR_list)):\n",
        "  print(str(comp_iteration[x]) + ' Components')\n",
        "  print('Absorption Ratio strategy:', ann_ret_AR[x], volatility_AR[x], sharpe_ratio_AR[x])\n",
        "  print('Equally weighted:', ann_ret_EW[x], volatility_EW[x], sharpe_ratio_EW[x])\n",
        "\n",
        "  \n",
        "  plt.title('Cumulative Portfolio Return for AR and EW Strategies')\n",
        "  plt.plot(returns_AR_list[x], label = 'Cumulative Portfolio Return ' + str(comp_iteration[x]) + ' Component AR Strategy')\n",
        "  plt.plot(returns_EQ_list[x], label = 'Cumulative Portfolio Return EW Strategy')\n",
        "  plt.xticks(rotation = 45)\n",
        "  plt.ylabel('Cumulative Portfolio Return')\n",
        "  plt.xlabel('Date')\n",
        "  plt.legend(loc = (1.05, 0.85))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "for x in range(0, len(ts_components_list)):\n",
        "  print(str(comp_iteration[x]) + ' Components')\n",
        "  pd.DataFrame(ts_components_list[x]).plot()\n",
        "  plt.show()\n",
        "\n",
        "for x in range(0, len(ts_absorb_ratio_list)):\n",
        "  print(str(comp_iteration[x]) + ' Components')\n",
        "  ts_absorb_ratio_list[x].plot(figsize=(12,6), title='Absorption Ratio via ' + method + ' (' + str(comp_iteration[x]) + ' Components)', linewidth=3)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "for x in range(0, len(ts_absorb_ratio_list)):\n",
        "  print(str(comp_iteration[x]) + ' Components')\n",
        "\n",
        "    \n",
        "  # following Kritzman and computing AR_delta = (15d_AR -1yr_AR) / sigma_AR\n",
        "  ts_ar = ts_absorb_ratio_list[x]\n",
        "  ar_mean_1yr = ts_ar.rolling(252).mean()\n",
        "  ar_mean_15d = ts_ar.rolling(15).mean()\n",
        "  ar_sd_1yr = ts_ar.rolling(252).std()\n",
        "  ar_delta = (ar_mean_15d - ar_mean_1yr) / ar_sd_1yr  # standardized shift in absorption ratio\n",
        "\n",
        "  df_plot = pd.DataFrame({'AR_delta': ar_delta.values, 'AR_1yr': ar_mean_1yr.values, 'AR_15d': ar_mean_15d.values}, \n",
        "                        index=ts_ar.index)\n",
        "  df_plot = df_plot.dropna()\n",
        "  if df_plot.shape[0] > 0:\n",
        "      df_plot.plot(figsize=(12, 6), title='Absorption Ratio Delta', linewidth=3, secondary_y=['AR_1yr','AR_15d'])\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  #NOTE: THE GRAPH AND RESULTS BELOW ARE ONLY APPROXIMATE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8GuFr5uZiyB"
      },
      "source": [
        "# Function to plot a metric as a function of number of decomposed \n",
        "# components used to calculate AR\n",
        "def metric_trend(metric_series, metric_name):\n",
        "\n",
        "  component_list = np.arange(1,len(metric_series) + 1)\n",
        "  plt.plot(component_list, metric_series)\n",
        "  plt.xticks(component_list)\n",
        "  if metric_name != 'AR Calculation Time':\n",
        "    plt.title(metric_name + ' vs. Number of Components (AR Strategy)')\n",
        "    plt.ylabel(metric_name)\n",
        "  else:\n",
        "    plt.title(metric_name + ' vs. Number of Components')   \n",
        "    plt.ylabel(metric_name + ' (s)')\n",
        "  plt.xlabel('Number of Components')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqY5KqopeZGl"
      },
      "source": [
        "# Function to plot a metric as a function of number of decomposed \n",
        "# components used to calculate AR to develop a trading strategy, compared to\n",
        "# the metric achieved by a baseline trading strategy - in our case, equally\n",
        "# weighted FI and EQ\n",
        "def metric_trend_compare(metric_series, metric_series_baseline, metric_name):\n",
        "\n",
        "  component_list = np.arange(1,len(metric_series) + 1)\n",
        "  plt.plot(component_list, metric_series, label = 'AR Strategy')\n",
        "  plt.plot(component_list, metric_series_baseline, label = 'EW Strategy')\n",
        "  plt.xticks(component_list)\n",
        "  plt.title(metric_name + ' vs. Number of Components')\n",
        "  plt.xlabel('Number of Components')\n",
        "  plt.ylabel(metric_name)\n",
        "\n",
        "  plt.legend(loc = (1.05,0.85))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbis8h-cb4lV"
      },
      "source": [
        "# Print metric results for different component counts\n",
        "metric_trend(sharpe_ratio_AR, 'Annualized Sharpe Ratio')\n",
        "metric_trend(volatility_AR, 'Annualized Volatility')\n",
        "metric_trend(ann_ret_AR, 'Annualized Return')\n",
        "metric_trend(AR_calculation_time, 'AR Calculation Time')\n",
        "metric_trend_compare(sharpe_ratio_AR, sharpe_ratio_EW, 'Annualized Sharpe Ratio')\n",
        "metric_trend_compare(volatility_AR, volatility_EW, 'Annualized Volatility')\n",
        "metric_trend_compare(ann_ret_AR, ann_ret_EW, 'Annualized Return')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge5oDyWk8B6V"
      },
      "source": [
        "# Tabulation of metric results\n",
        "df = pd.DataFrame([sharpe_ratio_AR, volatility_AR, ann_ret_AR, AR_calculation_time], ).T\n",
        "df.index = np.arange(1,comp_limit + 1)\n",
        "df.columns = ['SHARPE', 'VOL', 'ANN_RET', 'AR_CALC_TIME']\n",
        "df.sort_values(by = 'SHARPE', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}